{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOM:\n",
    "    def __init__(self, width, height, dim):\n",
    "        self.num_iters = 200\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.dim = dim\n",
    "        self.node_locs = self.get_locs()\n",
    "        \n",
    "        nodes = tf.Variable(tf.random_normal([width*height, dim]))\n",
    "        self.nodes = nodes\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, [dim])\n",
    "        iter = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.x = x\n",
    "        self.iter = iter\n",
    "        \n",
    "        bmu_loc = self.get_bmu_loc(x)\n",
    "        \n",
    "        self.propagate_nodes = self.get_propagation(bmu_loc, x, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_propagation(self, bmu_loc, x, iter):\n",
    "        num_nodes = self.width * self.height\n",
    "        rate = 1.0 - tf.div(iter, self.num_iters)\n",
    "        alpha = rate * 0.5\n",
    "        sigma = rate * tf.to_float(tf.maximum(self.width, self.height)) / 2.\n",
    "        expanded_bmu_loc = tf.expand_dims(tf.to_float(bmu_loc), 0)\n",
    "        sqr_dists_from_bmu = tf.reduce_sum(\n",
    "            tf.square(tf.subtract(expanded_bmu_loc, self.node_locs)), 1)\n",
    "        neigh_factor = tf.stack([tf.tile(tf.slice(rate, [i], [1]), [self.dim]) for i in range(num_nodes)])\n",
    "        nodes_diff = tf.multiply(\n",
    "            rate_factor,\n",
    "            tf.subtract(tf.stack([x for i in range(num_nodes)]), self.nodes))\n",
    "        update_nodes = tf.add(self.nodes, nodes_diff)\n",
    "        return tf.assign(self.nodes, update_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_bmu_loc(self, x):\n",
    "        expanded_x = tf.expand_dims(x, 0)\n",
    "        sqr_diff = tf.square(tf.subtract(expanded_x, self.nodes))\n",
    "        dists = tf.reduce_sum(sqr_diff, 1)\n",
    "        bmu_idx = tf.argmin(dists, 0)\n",
    "        bmu_loc = tf.stack([tf.mod(bmu_idx, self.width), tf.div(bmu_idx, self.width)])\n",
    "        return bmu_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_locs(self):\n",
    "        locs = [[x, y]\n",
    "               for y in range(self.height)\n",
    "               for x in range(self.width)]\n",
    "        return tf.to_float(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, data):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initalizer())\n",
    "            for i in range(self.num_iters):\n",
    "                for data_x in data:\n",
    "                    sess.run(self.propagate_nodes, feed_dict={self.x: data_x, self.iter: i})\n",
    "            centroid_grid = [[] for i in range(self.width)]\n",
    "            self.nodes_val = list(sess.run(self.nodes))\n",
    "            self.locs_val = list(sess.run(self.nodes_locs))\n",
    "            for i, l in enumerate(self.locs_val):\n",
    "                centroid_grid[int(1[0])].append(self.nodes_val[i])\n",
    "            self.centroid_grid = centroid_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training...\n",
      " random_initialization took: 0.000000 seconds\n",
      " Rough training...\n",
      " radius_ini: 2.000000 , radius_final: 1.000000, trainlen: 40\n",
      "\n",
      " epoch: 1 ---> elapsed time:  0.107000, quantization error: 0.872350\n",
      "\n",
      " epoch: 2 ---> elapsed time:  0.106000, quantization error: 1.347228\n",
      "\n",
      " epoch: 3 ---> elapsed time:  0.106000, quantization error: 1.188281\n",
      "\n",
      " epoch: 4 ---> elapsed time:  0.104000, quantization error: 1.165931\n",
      "\n",
      " epoch: 5 ---> elapsed time:  0.105000, quantization error: 1.155589\n",
      "\n",
      " epoch: 6 ---> elapsed time:  0.104000, quantization error: 1.149338\n",
      "\n",
      " epoch: 7 ---> elapsed time:  0.105000, quantization error: 1.143093\n",
      "\n",
      " epoch: 8 ---> elapsed time:  0.106000, quantization error: 1.136857\n",
      "\n",
      " epoch: 9 ---> elapsed time:  0.104000, quantization error: 1.130637\n",
      "\n",
      " epoch: 10 ---> elapsed time:  0.104000, quantization error: 1.124441\n",
      "\n",
      " epoch: 11 ---> elapsed time:  0.103000, quantization error: 1.118276\n",
      "\n",
      " epoch: 12 ---> elapsed time:  0.103000, quantization error: 1.112151\n",
      "\n",
      " epoch: 13 ---> elapsed time:  0.102000, quantization error: 1.106075\n",
      "\n",
      " epoch: 14 ---> elapsed time:  0.105000, quantization error: 1.100058\n",
      "\n",
      " epoch: 15 ---> elapsed time:  0.106000, quantization error: 1.094108\n",
      "\n",
      " epoch: 16 ---> elapsed time:  0.104000, quantization error: 1.088238\n",
      "\n",
      " epoch: 17 ---> elapsed time:  0.107000, quantization error: 1.081073\n",
      "\n",
      " epoch: 18 ---> elapsed time:  0.107000, quantization error: 1.076848\n",
      "\n",
      " epoch: 19 ---> elapsed time:  0.105000, quantization error: 1.062233\n",
      "\n",
      " epoch: 20 ---> elapsed time:  0.106000, quantization error: 1.053589\n",
      "\n",
      " epoch: 21 ---> elapsed time:  0.104000, quantization error: 1.044949\n",
      "\n",
      " epoch: 22 ---> elapsed time:  0.106000, quantization error: 1.036336\n",
      "\n",
      " epoch: 23 ---> elapsed time:  0.105000, quantization error: 1.027720\n",
      "\n",
      " epoch: 24 ---> elapsed time:  0.106000, quantization error: 1.006000\n",
      "\n",
      " epoch: 25 ---> elapsed time:  0.106000, quantization error: 0.996740\n",
      "\n",
      " epoch: 26 ---> elapsed time:  0.107000, quantization error: 0.987613\n",
      "\n",
      " epoch: 27 ---> elapsed time:  0.106000, quantization error: 0.978650\n",
      "\n",
      " epoch: 28 ---> elapsed time:  0.104000, quantization error: 0.969884\n",
      "\n",
      " epoch: 29 ---> elapsed time:  0.105000, quantization error: 0.961340\n",
      "\n",
      " epoch: 30 ---> elapsed time:  0.103000, quantization error: 0.953041\n",
      "\n",
      " epoch: 31 ---> elapsed time:  0.105000, quantization error: 0.945000\n",
      "\n",
      " epoch: 32 ---> elapsed time:  0.107000, quantization error: 0.936057\n",
      "\n",
      " epoch: 33 ---> elapsed time:  0.103000, quantization error: 0.925003\n",
      "\n",
      " epoch: 34 ---> elapsed time:  0.106000, quantization error: 0.916002\n",
      "\n",
      " epoch: 35 ---> elapsed time:  0.105000, quantization error: 0.907154\n",
      "\n",
      " epoch: 36 ---> elapsed time:  0.104000, quantization error: 0.898450\n",
      "\n",
      " epoch: 37 ---> elapsed time:  0.105000, quantization error: 0.889871\n",
      "\n",
      " epoch: 38 ---> elapsed time:  0.105000, quantization error: 0.881044\n",
      "\n",
      " epoch: 39 ---> elapsed time:  0.107000, quantization error: 0.865540\n",
      "\n",
      " epoch: 40 ---> elapsed time:  0.103000, quantization error: 0.855160\n",
      "\n",
      " Finetune training...\n",
      " radius_ini: 1.000000 , radius_final: 1.000000, trainlen: 67\n",
      "\n",
      " epoch: 1 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 2 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 3 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 4 ---> elapsed time:  0.102000, quantization error: 0.844634\n",
      "\n",
      " epoch: 5 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 6 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 7 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 8 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 9 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 10 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 11 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 12 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 13 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 14 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 15 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 16 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 17 ---> elapsed time:  0.102000, quantization error: 0.844634\n",
      "\n",
      " epoch: 18 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 19 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 20 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 21 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 22 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 23 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 24 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 25 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 26 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 27 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 28 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 29 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 30 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 31 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 32 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 33 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 34 ---> elapsed time:  0.102000, quantization error: 0.844634\n",
      "\n",
      " epoch: 35 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 36 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 37 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 38 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 39 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 40 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 41 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 42 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 43 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 44 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 45 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 46 ---> elapsed time:  0.102000, quantization error: 0.844634\n",
      "\n",
      " epoch: 47 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 48 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 49 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 50 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 51 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 52 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 53 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 54 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 55 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 56 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 57 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 58 ---> elapsed time:  0.106000, quantization error: 0.844634\n",
      "\n",
      " epoch: 59 ---> elapsed time:  0.107000, quantization error: 0.844634\n",
      "\n",
      " epoch: 60 ---> elapsed time:  0.103000, quantization error: 0.844634\n",
      "\n",
      " epoch: 61 ---> elapsed time:  0.102000, quantization error: 0.844634\n",
      "\n",
      " epoch: 62 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 63 ---> elapsed time:  0.102000, quantization error: 0.844634\n",
      "\n",
      " epoch: 64 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 65 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " epoch: 66 ---> elapsed time:  0.105000, quantization error: 0.844634\n",
      "\n",
      " epoch: 67 ---> elapsed time:  0.104000, quantization error: 0.844634\n",
      "\n",
      " Final quantization error: 0.844634\n",
      " train took: 11.306000 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113e31b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sompy.visualization.mapview import View2D\n",
    "import sompy\n",
    "\n",
    "colors = np.array(\n",
    "         [[0., 0., 0.],\n",
    "          [0., 0., 1.],\n",
    "          [0., 0., 0.5],\n",
    "          [0.125, 0.529, 1.0],\n",
    "          [0.33, 0.4, 0.67],\n",
    "          [0.6, 0.5, 1.0],\n",
    "          [0., 1., 0.],\n",
    "          [1., 0., 0.],\n",
    "          [0., 1., 1.],\n",
    "          [1., 0., 1.],\n",
    "          [1., 1., 0.],\n",
    "          [1., 1., 1.],\n",
    "          [.33, .33, .33],\n",
    "          [.5, .5, .5],\n",
    "          [.66, .66, .66]])\n",
    "\n",
    "\n",
    "\n",
    "mapsize = [4, 4]\n",
    "\n",
    "som = SOMFactory().build(colors, normalization = 'var', initialization='random')\n",
    "\n",
    "som.train(n_job=1, verbose='info')\n",
    "\n",
    "v = sompy.mapview.View2DPacked(50, 50, 'test',text_size=8)  \n",
    "v.show(som, what='codebook', which_dim=\"all\", cmap=None, col_sz=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
