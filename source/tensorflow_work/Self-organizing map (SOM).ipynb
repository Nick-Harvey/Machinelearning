{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOM:\n",
    "    def __init__(self, width, height, dim):\n",
    "        self.num_iters = 200\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.dim = dim\n",
    "        self.node_locs = self.get_locs()\n",
    "        \n",
    "        nodes = tf.Variable(tf.random_normal([width*height, dim]))\n",
    "        self.nodes = nodes\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, [dim])\n",
    "        iter = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.x = x\n",
    "        self.iter = iter\n",
    "        \n",
    "        bmu_loc = self.get_bmu_loc(x)\n",
    "        \n",
    "        self.propagate_nodes = self.get_propagation(bmu_loc, x, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_propagation(self, bmu_loc, x, iter):\n",
    "        num_nodes = self.width * self.height\n",
    "        rate = 1.0 - tf.div(iter, self.num_iters)\n",
    "        alpha = rate * 0.5\n",
    "        sigma = rate * tf.to_float(tf.maximum(self.width, self.height)) / 2.\n",
    "        expanded_bmu_loc = tf.expand_dims(tf.to_float(bmu_loc), 0)\n",
    "        sqr_dists_from_bmu = tf.reduce_sum(\n",
    "            tf.square(tf.subtract(expanded_bmu_loc, self.node_locs)), 1)\n",
    "        neigh_factor = tf.stack([tf.tile(tf.slice(rate, [i], [1]), [self.dim]) for i in range(num_nodes)])\n",
    "        nodes_diff = tf.multiply(\n",
    "            rate_factor,\n",
    "            tf.subtract(tf.stack([x for i in range(num_nodes)]), self.nodes))\n",
    "        update_nodes = tf.add(self.nodes, nodes_diff)\n",
    "        return tf.assign(self.nodes, update_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_bmu_loc(self, x):\n",
    "        expanded_x = tf.expand_dims(x, 0)\n",
    "        sqr_diff = tf.square(tf.subtract(expanded_x, self.nodes))\n",
    "        dists = tf.reduce_sum(sqr_diff, 1)\n",
    "        bmu_idx = tf.argmin(dists, 0)\n",
    "        bmu_loc = tf.stack([tf.mod(bmu_idx, self.width), tf.div(bmu_idx, self.width)])\n",
    "        return bmu_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_locs(self):\n",
    "        locs = [[x, y]\n",
    "               for y in range(self.height)\n",
    "               for x in range(self.width)]\n",
    "        return tf.to_float(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, data):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initalizer())\n",
    "            for i in range(self.num_iters):\n",
    "                for data_x in data:\n",
    "                    sess.run(self.propagate_nodes, feed_dict={self.x: data_x, self.iter: i})\n",
    "            centroid_grid = [[] for i in range(self.width)]\n",
    "            self.nodes_val = list(sess.run(self.nodes))\n",
    "            self.locs_val = list(sess.run(self.nodes_locs))\n",
    "            for i, l in enumerate(self.locs_val):\n",
    "                centroid_grid[int(1[0])].append(self.nodes_val[i])\n",
    "            self.centroid_grid = centroid_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training...\n",
      " random_initialization took: 0.000000 seconds\n",
      " Rough training...\n",
      " radius_ini: 2.000000 , radius_final: 1.000000, trainlen: 40\n",
      "\n",
      " epoch: 1 ---> elapsed time:  0.103000, quantization error: 0.722714\n",
      "\n",
      " epoch: 2 ---> elapsed time:  0.105000, quantization error: 1.343395\n",
      "\n",
      " epoch: 3 ---> elapsed time:  0.105000, quantization error: 1.182127\n",
      "\n",
      " epoch: 4 ---> elapsed time:  0.105000, quantization error: 1.172766\n",
      "\n",
      " epoch: 5 ---> elapsed time:  0.105000, quantization error: 1.166492\n",
      "\n",
      " epoch: 6 ---> elapsed time:  0.106000, quantization error: 1.156793\n",
      "\n",
      " epoch: 7 ---> elapsed time:  0.107000, quantization error: 1.151126\n",
      "\n",
      " epoch: 8 ---> elapsed time:  0.105000, quantization error: 1.145475\n",
      "\n",
      " epoch: 9 ---> elapsed time:  0.106000, quantization error: 1.139849\n",
      "\n",
      " epoch: 10 ---> elapsed time:  0.105000, quantization error: 1.134253\n",
      "\n",
      " epoch: 11 ---> elapsed time:  0.105000, quantization error: 1.128694\n",
      "\n",
      " epoch: 12 ---> elapsed time:  0.105000, quantization error: 1.123177\n",
      "\n",
      " epoch: 13 ---> elapsed time:  0.106000, quantization error: 1.117707\n",
      "\n",
      " epoch: 14 ---> elapsed time:  0.105000, quantization error: 1.112288\n",
      "\n",
      " epoch: 15 ---> elapsed time:  0.105000, quantization error: 1.106721\n",
      "\n",
      " epoch: 16 ---> elapsed time:  0.107000, quantization error: 1.104354\n",
      "\n",
      " epoch: 17 ---> elapsed time:  0.107000, quantization error: 1.097981\n",
      "\n",
      " epoch: 18 ---> elapsed time:  0.102000, quantization error: 1.091621\n",
      "\n",
      " epoch: 19 ---> elapsed time:  0.107000, quantization error: 1.085280\n",
      "\n",
      " epoch: 20 ---> elapsed time:  0.105000, quantization error: 1.078963\n",
      "\n",
      " epoch: 21 ---> elapsed time:  0.103000, quantization error: 1.072673\n",
      "\n",
      " epoch: 22 ---> elapsed time:  0.107000, quantization error: 1.066413\n",
      "\n",
      " epoch: 23 ---> elapsed time:  0.105000, quantization error: 1.059997\n",
      "\n",
      " epoch: 24 ---> elapsed time:  0.103000, quantization error: 1.052956\n",
      "\n",
      " epoch: 25 ---> elapsed time:  0.105000, quantization error: 1.046528\n",
      "\n",
      " epoch: 26 ---> elapsed time:  0.105000, quantization error: 1.038517\n",
      "\n",
      " epoch: 27 ---> elapsed time:  0.102000, quantization error: 1.030348\n",
      "\n",
      " epoch: 28 ---> elapsed time:  0.103000, quantization error: 1.022122\n",
      "\n",
      " epoch: 29 ---> elapsed time:  0.105000, quantization error: 1.013840\n",
      "\n",
      " epoch: 30 ---> elapsed time:  0.104000, quantization error: 1.005498\n",
      "\n",
      " epoch: 31 ---> elapsed time:  0.104000, quantization error: 0.997093\n",
      "\n",
      " epoch: 32 ---> elapsed time:  0.106000, quantization error: 0.988620\n",
      "\n",
      " epoch: 33 ---> elapsed time:  0.104000, quantization error: 0.980073\n",
      "\n",
      " epoch: 34 ---> elapsed time:  0.107000, quantization error: 0.971446\n",
      "\n",
      " epoch: 35 ---> elapsed time:  0.105000, quantization error: 0.962731\n",
      "\n",
      " epoch: 36 ---> elapsed time:  0.107000, quantization error: 0.953919\n",
      "\n",
      " epoch: 37 ---> elapsed time:  0.106000, quantization error: 0.945001\n",
      "\n",
      " epoch: 38 ---> elapsed time:  0.104000, quantization error: 0.935967\n",
      "\n",
      " epoch: 39 ---> elapsed time:  0.107000, quantization error: 0.926802\n",
      "\n",
      " epoch: 40 ---> elapsed time:  0.107000, quantization error: 0.917492\n",
      "\n",
      " Finetune training...\n",
      " radius_ini: 1.000000 , radius_final: 1.000000, trainlen: 67\n",
      "\n",
      " epoch: 1 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 2 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 3 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 4 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 5 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 6 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 7 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 8 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 9 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 10 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 11 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 12 ---> elapsed time:  0.103000, quantization error: 0.908020\n",
      "\n",
      " epoch: 13 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 14 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 15 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 16 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 17 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 18 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 19 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 20 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 21 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 22 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 23 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 24 ---> elapsed time:  0.107000, quantization error: 0.908020\n",
      "\n",
      " epoch: 25 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 26 ---> elapsed time:  0.103000, quantization error: 0.908020\n",
      "\n",
      " epoch: 27 ---> elapsed time:  0.107000, quantization error: 0.908020\n",
      "\n",
      " epoch: 28 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 29 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 30 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 31 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 32 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 33 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 34 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 35 ---> elapsed time:  0.102000, quantization error: 0.908020\n",
      "\n",
      " epoch: 36 ---> elapsed time:  0.103000, quantization error: 0.908020\n",
      "\n",
      " epoch: 37 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 38 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 39 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 40 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 41 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 42 ---> elapsed time:  0.102000, quantization error: 0.908020\n",
      "\n",
      " epoch: 43 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 44 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 45 ---> elapsed time:  0.103000, quantization error: 0.908020\n",
      "\n",
      " epoch: 46 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 47 ---> elapsed time:  0.107000, quantization error: 0.908020\n",
      "\n",
      " epoch: 48 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 49 ---> elapsed time:  0.107000, quantization error: 0.908020\n",
      "\n",
      " epoch: 50 ---> elapsed time:  0.102000, quantization error: 0.908020\n",
      "\n",
      " epoch: 51 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 52 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 53 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 54 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 55 ---> elapsed time:  0.107000, quantization error: 0.908020\n",
      "\n",
      " epoch: 56 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 57 ---> elapsed time:  0.103000, quantization error: 0.908020\n",
      "\n",
      " epoch: 58 ---> elapsed time:  0.103000, quantization error: 0.908020\n",
      "\n",
      " epoch: 59 ---> elapsed time:  0.102000, quantization error: 0.908020\n",
      "\n",
      " epoch: 60 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 61 ---> elapsed time:  0.106000, quantization error: 0.908020\n",
      "\n",
      " epoch: 62 ---> elapsed time:  0.104000, quantization error: 0.908020\n",
      "\n",
      " epoch: 63 ---> elapsed time:  0.107000, quantization error: 0.908020\n",
      "\n",
      " epoch: 64 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 65 ---> elapsed time:  0.105000, quantization error: 0.908020\n",
      "\n",
      " epoch: 66 ---> elapsed time:  0.102000, quantization error: 0.908020\n",
      "\n",
      " epoch: 67 ---> elapsed time:  0.107000, quantization error: 0.908020\n",
      "\n",
      " Final quantization error: 0.908020\n",
      " train took: 11.306000 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAAEMCAYAAAALY999AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADOlJREFUeJzt3X2MpWdZx/HfBQtU+kqMGtQQGjBVk7JVK1Q0sPwBVdQCuptFE4EAobuElFSbkhgwG0Ijbw2kUan2j0IiSl+ARUKMRhSh0tBq37SJuiaYilJiKoGWNbSEmz/mdDsz1vTM8HSeOXt9PkmTOS/P/Vxnemc6333OntYYIwAAAHCye8LcAwAAAMBOEMAAAAC0IIABAABoQQADAADQggAGAACgBQEMAABACwIYAHZQVZ1VVa/fxnH7qur8x2MmAOhCAAPAzjoryZYDOMm+JAIYAL4LNcaYewYAaKOqPpzk5UmOJflwkh9O8vwkT0rytjHGJ6vqlUl+J8lDSf41yW8muXVx+74kB8cY/zLD+ACw0gQwAOygqnpmko+MMS6oqkNJThljvL+qzkhyc5LzktyW5KIxxher6swxxteq6kiSe8cYV881OwCsOm+BBoD5vDjJxVV1R5LPJjk9yQ8m+bsk11TVa5P4k2oAmMieuQcAgMYqyevGGJ/fcGfV4SQ/k+SiJF+oqnPnGA4ATjauAAPAzro/a1d6k+SvkhyuqickSVWdt7j/7EUU/3bW/m7w6ZuOAwC2QQADwA4aY9yX5K6quitrQfuVJHdW1d1Z++CrJLmyqv4xyV1JPjTG+GqSTyb5jaq6varOmWN2AFh1PgQLAACAFlwBBgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAtCGAAAABaEMAAAAC0IIABAABoQQAvoar+tqqev+m+P6mqA0sef6iq9j/Gcz5TVT/6KPd/sKp+fguz/lRV/X1VPbSV49g9Vmy/Haqqu6vqrqo6WlVnLHss81uxvfayxT67o6puqqpzlj0WAOBhAng5NyQ58UteVZ2S5EVJPvVYB1bVE8cYV48xbnwc51vv3iSvT/KnO3Q+prdK++2fkvz0GOM5i68v3aHzMo1V2mufTrJ3jHFekncm+d0dOi8AcBIRwMu5MckrqqoWty9M8rkke6vq5qq6var+uqqeniRVdaSqrq2qm5O8e3H70OKxQ1V16+JKxrVVtf7fwcVVdefi8WdvHqKqnltVn6uq26rqhqp66ubnjDH+c4xxR5JvT/w9YOes0n67aYxxfHHzH5L80HTfBnbAKu21B8YYY3Hz1CRj83MAAB6LAF7CGOPeJPcked7irgNJrk9yd5KfG2P8RJKrk1y+7rBnJXnBGOO3Ni13/Rjj4Stm/5vklzeeauxN8o4k719/UFU9Ocl7klw0xvjJJLckeeMUr4/dZYX326uydpWOFbFqe62qDlbVsSRXJnnLVl8vAMCeuQdYIdcnOVBVt2XtLYJvSPJ9Sf64qs7O2vfynnXPPzrGeOhR1tlbVe9IckaSs5L8R5JPLB77SJKMMT5RVX+w6bhzkjwnyd8sLtY8OWLjZLZS+62q3phkzxjjui29SnaDldlri/11XVVdlORtSV69xdcKADQngJf30SSfT/KZJDeNMY5X1duTfGyM8cGqOj/Je9c9//ijrJEk1yT5hTHGsaq6LMlp6x4b/8/XSVJJbh1jvGTDnWsfQPPWxc2XjjH+aysvil1rZfZbVb0ka9H0guVfHrvIyuy1EwuM8WdV9YfLvTwAgEd4C/SS1r1V8IqsfXBMsnal48uLr1+z5FKnJvnvxYfNHNz02MEkqapfSnL7psf+OcnZVXXu4jmnVtWzxxg3jjHOW/wjfk8Sq7LfqurHk/xekpePMb6+/Ctkt1ihvfash/+uclW9KImfdwDAlrkCvDU3JHlXHvmE1PcmubaqHkjyl0uucUWS25J8Jf/3F8EnVtWdSR5M8mvrHxhjPFhVv57k6qo6LWtXTS5P8m/rn1dr/2uQTyd5WpJfrKq7xxj7lpyN3WXX77ckb09yZpKjizb57BjjkiVnY/dYhb22P8mrquqhJF/L8mEOAHBCPfKhmgAAAHDy8hZoAAAAWhDAAAAAtCCAAQAAaEEAAwAA0IIABgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAtCGAAAABaEMAAAAC0IIABAABoQQADAADQggAGAACgBQEMAABACwIYAACAFgQwAAAALQhgAAAAWhDAAAAAtCCAAQAAaEEAAwAA0IIABgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAtCGAAAABaEMAAAAC0IIABAABoQQADAADQggAGAACgBQEMAABACwIYAACAFgQwAAAALQhgAAAAWhDAAAAAtCCAAQAAaEEAAwAA0IIABgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFrYs5MnO1I1pljnB6ZYZCKHr5h7go3qwkm+xdM4/8gky4xxpLZz3Mfy0km+Ge/KW6ZYZhK3nP3CuUfY6N+/MPcE6/z5JKtsZ7+9Jh+YZK996KOHp1hmGvuvmnuCTf5n7gHWmea/QmMc3tbPNgBg+1wBBgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAtCGAAAABaEMAAAAC0IIABAABoQQADAADQggAGAACgBQEMAABACwIYAACAFgQwAAAALQhgAAAAWhDAAAAAtCCAAQAAaEEAAwAA0IIABgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALe3byZEcOTLTQz060zgQ+8OZXzz3CRpfOPcB63zPr2S/85l9Mss79Tzl9knWmcMtNz5t7hI3etIvmOfr9s53697/5pknWecqvPjjJOlP4o1e+ee4RNvrW3AOsc8HcAwAA2+UKMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAtCGAAAABaEMAAAAC0IIABAABoQQADAADQggAGAACgBQEMAABACwIYAACAFgQwAAAALQhgAAAAWhDAAAAAtCCAAQAAaEEAAwAA0IIABgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAtCGAAAABaEMAAAAC0UGOMHTvZrTl3kpNdl4NTLDOJKz/11rlH2OjQ3AOs86WrJllmjEtqWwf+Sk2zuS+YZJVJ3Hn5j8w9wgYHcsPcI5xw7NK9k6wz3pct77fjp02z18Z9u+fPJJ/2wFfnHmGDg9973dwjnHDNN94wyTqnnDq297MNANi23fPbFgAAADyOBDAAAAAtCGAAAABaEMAAAAC0IIABAABoQQADAADQggAGAACgBQEMAABACwIYAACAFgQwAAAALQhgAAAAWhDAAAAAtCCAAQAAaEEAAwAA0IIABgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAtCGAAAABaEMAAAAC0IIABAABoQQADAADQggAGAACgBQEMAABACwIYAACAFgQwAAAALezZyZPdmP2TrHPVfZdMss4kLpt7gE2+9MW5J1jn/lnPfuTj06yzb6J1prDvzGNzj7DBKy4+OvcIJ7x7/975zv2NadY5cs+3p1loAk99xvG5R9jgsrxn7hFOOOXHJlrononWAQCW5gowAAAALQhgAAAAWhDAAAAAtCCAAQAAaEEAAwAA0IIABgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAt1Bhj7hkAAADgcecKMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAtCGAAAABaEMAAAAC0IIABAABoQQADAADQggAGAACgBQEMAABACwIYAACAFgQwAAAALQhgAAAAWhDAAAAAtCCAAQAAaEEAAwAA0IIABgAAoAUBDAAAQAsCGAAAgBYEMAAAAC0IYAAAAFoQwAAAALQggAEAAGhBAAMAANCCAAYAAKAFAQwAAEALAhgAAIAWBDAAAAAtCGAAAABaEMAAAAC0IIABAABoQQADAADQggAGAACgBQEMAABACwIYAACAFgQwAAAALQhgAAAAWhDAAAAAtCCAAQAAaEEAAwAA0IIABgAAoIXvALP+2QbWrjtLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111e1c5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sompy.visualization.mapview import View2D\n",
    "import sompy\n",
    "\n",
    "colors = np.array(\n",
    "         [[0., 0., 0.],\n",
    "          [0., 0., 1.],\n",
    "          [0., 0., 0.5],\n",
    "          [0.125, 0.529, 1.0],\n",
    "          [0.33, 0.4, 0.67],\n",
    "          [0.6, 0.5, 1.0],\n",
    "          [0., 1., 0.],\n",
    "          [1., 0., 0.],\n",
    "          [0., 1., 1.],\n",
    "          [1., 0., 1.],\n",
    "          [1., 1., 0.],\n",
    "          [1., 1., 1.],\n",
    "          [.33, .33, .33],\n",
    "          [.5, .5, .5],\n",
    "          [.66, .66, .66]])\n",
    "\n",
    "\n",
    "#som = sompy.SOMFactory.build(4, 4, 3)\n",
    "mapsize = [20, 30]\n",
    "#som = sompy.SOMFactory.build(colors, mapsize, mask=None, mapshape='planar', lattice='rect', normalization='var', initialization='random', neighborhood='gaussian', training='batch', name='sompy')\n",
    "\n",
    "som = SOMFactory().build(colors, normalization = 'var', initialization='random')\n",
    "\n",
    "som.train(n_job=3, verbose='info')\n",
    "\n",
    "v = sompy.mapview.View2DPacked(50, 50, 'test',text_size=8)  \n",
    "v.show(som, what='codebook', which_dim='all', cmap='jet', col_sz=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
